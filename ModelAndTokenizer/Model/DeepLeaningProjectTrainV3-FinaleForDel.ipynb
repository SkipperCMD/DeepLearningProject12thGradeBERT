{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "#tf libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "#torch libraries\n",
    "import torch\n",
    "from torch import cuda\n",
    "\n",
    "#Time related libraries\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#General libraries\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Dealing with files, data and general stuff\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "#May be used but very unlikely.\n",
    "from subprocess import check_output\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from numba import jit, cuda #Former way of GPU acceleration in my project.\n",
    "\n",
    "#Transformers Library. Taken from \"Hugging Face\".\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForMaskedLM, BertTokenizerFast, pipeline\n",
    "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "#Optimizers taken from the \"Transformers\" Library that I have mentioned above as well.\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cthulhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1706: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('G:/VisualStudioCodeG/Resources/Tokenizers/bookcorpus/Run1/BERTbookcorpusTokenizerRun1-vocab.txt', max_length=512)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To shorten overall runtime, provide convinience and reduce the probability of the kernel crashing; The tokenization of the arrays is done within a different script.\n",
    "# Please refer to the appropriate tokenization script for tokenizing and saving the list, Once you have done so, You may come back here to load the tokenized data and proceed with the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle a tokenized array (Load) [Multiple]\n",
    "#inputs = []\n",
    "#num = 0\n",
    "\n",
    "#for i in tqdm(range(num)):\n",
    "    #with open(f'G:/VisualStudioCodeG/Resources/PickleSaveArrays/inputsArrayRun1/tokenizedText_{i}.txt', \"rb\") as fp: # rb - Read Binary\n",
    "        #inputsData = pickle.load(fp)\n",
    "        #inputs.extend(inputsData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickle a tokenized array (Load) [Single]\n",
    "num = 0 # Index number for the txt file\n",
    "\n",
    "with open(f'G:/VisualStudioCodeG/Resources/PickleSaveArrays/inputsArrayRun1/tokenizedText_{num}.txt', \"rb\") as fp: # rb - Read Binary\n",
    "    inputs = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [PAD] = 0 | [UNK] = 1 | [CLS] = 2 | [SEP] = 3 | [MASK] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a random array of floats with dimensions equal to those of input_ids.\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# Masking 15% of the random array. Where the token is not 0 == [PAD], 2 == [CLS], or 3 == [SEP]\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 2) * \\\n",
    "           (inputs.input_ids != 3) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 93263.90it/s]\n"
     ]
    }
   ],
   "source": [
    "selection = []\n",
    "\n",
    "for i in tqdm(range(inputs.input_ids.shape[0])):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:01<00:00, 80386.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(inputs.input_ids.shape[0])):\n",
    "    inputs.input_ids[i, selection[i]] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Return dictionary* of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "# Creating a Dataset object for the loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing a BERT configuration\n",
    "configuration = BertConfig(\n",
    "    vocab_size=30_522,  # Changing vocab_size to the size of my tokenizer's vocabulary. The rest is set to BERT's default parameters (Except the type_vocab_size, The reason is provided below).\n",
    "    max_position_embeddings=512,\n",
    "    hidden_size=768,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=12,\n",
    "    type_vocab_size=1 # I am not using token_type_ids in my model at all. Hence, I am changing the default value from 2 to 1 so that there won't be issues later down the run.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 21 07:44:26 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 527.41       Driver Version: 527.41       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "| 53%   35C    P8    55W / 390W |   1305MiB / 24576MiB |     13%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       972    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A      3376    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6468    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6768    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      7692    C+G   ...obeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A      7952    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A      9516    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     12736    C+G   ...3d8bbwe\\CalculatorApp.exe    N/A      |\n",
      "|    0   N/A  N/A     13576    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14148    C+G   ...bbwe\\PaintStudio.View.exe    N/A      |\n",
      "|    0   N/A  N/A     15384    C+G   ...ograms\\Opera GX\\opera.exe    N/A      |\n",
      "|    0   N/A  N/A     16436    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     17068    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     21688    C+G   ... Host\\Razer Synapse 3.exe    N/A      |\n",
      "|    0   N/A  N/A     24044    C+G   ...arp.BrowserSubprocess.exe    N/A      |\n",
      "|    0   N/A  N/A     25672    C+G   ...\\app-1.0.9011\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     28744    C+G   ...cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     29084    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A     35552    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "#Setting the device to be the GPU, Utilizating the Cuda cores on the GPU. If however, It can not find an available Cuda Device (GPU in this case) to connect to, It'll set the device to be the system's CPU.\n",
    "#After setting the device, I'm moving the model over to that device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cthulhu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Activating the model on training mode, As well as initializing the Optimizer.\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5) \n",
    "\n",
    "# Using a AdamW (Also known as Adam with Weighted Decay) with a learning rate of 5e-5 []\n",
    "# The reason for using AdamW and not regular Adam as a starting point is because AdamW is a part of the transformers library, And because the weight decay defaults to 0: Unless I change it's value, Weighted Decay will not be used. \n",
    "# The reason for using AdamW (with Weight decay disabled) and not Adam is so that if in the future, There would come a need for using weighted decay, A change of value is all that would be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Checks + Checking what device we are running on;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing variables to store the loss history at\n",
    "y_loss = {} # Loss History\n",
    "x_epoch = [] # The epoch at which the loss was at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 6250/6250 [48:26<00:00,  2.15it/s, loss=0.0174] \n",
      "Epoch 1: 100%|██████████| 6250/6250 [47:03<00:00,  2.21it/s, loss=0.0194]  \n"
     ]
    }
   ],
   "source": [
    "path_To_Save = \"G:/VisualStudioCodeG/Resources/Models/bookcorpus/Run2\"\n",
    "epochs = 2\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #setting up a loop using TQDM and the dataloader;\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "        optimizer.zero_grad() #Taking the gradients from the previous step into consideration.\n",
    "\n",
    "        #Getting all the tensor batches required for the training process, And putting them on the device;\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        #Model Training Process;\n",
    "        modelOutputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        #Extracting the loss from the model output and running the backward propagation;\n",
    "        loss = modelOutputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        #Updating the parameters;\n",
    "        optimizer.step()\n",
    "\n",
    "        # Updating the TensorBoard graph;\n",
    "        #running_loss += loss.item()\n",
    "\n",
    "        # ...log the running loss\n",
    "        x_epoch.append(loss.item())\n",
    "        running_loss = 0.0\n",
    "\n",
    "        #Printing out the important information to the progress bar for the User's Sake;\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    y_loss[epoch] = x_epoch\n",
    "\n",
    "\n",
    "model.save_pretrained(path_To_Save) #Saving the model I have trained to the path I have specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'G:\\VisualStudioCodeG\\Resources\\Graphs\\RunFinale\\PickleThings\\lossHistory2.txt', \"wb\") as fp: # wb - Write Binary\n",
    "        pickle.dump(y_loss, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Training Loss\")\n",
    "plt.plot(x_epoch,label=\"train\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mg:\\VisualStudioCodeG\\FinalProjectDL\\ImportantV3\\DeepLeaningProjectTrainV3-Finale.ipynb Cell 32\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/VisualStudioCodeG/FinalProjectDL/ImportantV3/DeepLeaningProjectTrainV3-Finale.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m plt\u001b[39m.\u001b[39mplot(y_loss[x_epoch])\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "#plt.plot(y_loss[x_epoch]) #Ignore this line, It's not valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If training fails due to GPU memory;\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after running the command above (in case of failure) flush the VRAM by closing and reopening visual studio code. (Or Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() #Turning on evaluation mode, As I am done training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
