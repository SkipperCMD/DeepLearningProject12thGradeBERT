{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basics\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "#torch libraries\n",
    "import torch\n",
    "from torch import cuda\n",
    "\n",
    "#Time related libraries\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "#General libraries\n",
    "import collections\n",
    "import math\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Dealing with files, data and general stuff\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "#May be used but very unlikely.\n",
    "from subprocess import check_output\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from numba import jit, cuda #Former way of GPU acceleration in my project.\n",
    "\n",
    "#Transformers Library. Taken from \"Hugging Face\".\n",
    "from transformers import BertConfig, BertModel, BertTokenizer, BertForMaskedLM, BertTokenizerFast, pipeline\n",
    "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "#Optimizers taken from the \"Transformers\" Library that I have mentioned above as well.\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tokenizer = \"G:/VisualStudioCodeG/Resources/Tokenizers/bookcorpus/Run1/BERTbookcorpusTokenizerRun1-vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast(path_to_tokenizer, max_length=512, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = \"G:/VisualStudioCodeG/Resources/PickleSaveArrays/inputsArrayRun1/text_{count}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('G:/VisualStudioCodeG/Resources/DataTestHuggingFace/bookcorpus/').glob('**/*.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 741/741 [00:16<00:00, 45.59it/s]\n"
     ]
    }
   ],
   "source": [
    "linesA = []\n",
    "\n",
    "fileList = glob.glob('G:/VisualStudioCodeG/Resources/DataTestHuggingFace/bookcorpus' + '/*.txt')\n",
    "for filePath in tqdm(fileList):\n",
    "    with open(filePath, 'r', encoding='utf-8') as textFile:\n",
    "        linesData = textFile.read().split('\\n')\n",
    "        linesA.extend(linesData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fileList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "count = 0\n",
    "\n",
    "for path in tqdm(paths):\n",
    "    with open(path, 'r', encoding='utf-8') as textFile:\n",
    "        lines = textFile.read().split('\\n')\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "    text=lines,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt')\n",
    "\n",
    "    # Pickle a tokenized array (Save) - Splits\n",
    "    with open(f'G:/VisualStudioCodeG/Resources/PickleSaveArrays/inputsArrayRun1/tokenizedText_{count}.txt', \"wb\") as fp: # wb - Write Binary\n",
    "        pickle.dump(inputs, fp)\n",
    "        lines = []\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\n",
    "    text=linesA,\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding='max_length',\n",
    "    return_tensors='pt')\n",
    "\n",
    "# Pickle a tokenized array (Save) - Whole\n",
    "with open(\"G:/VisualStudioCodeG/Resources/PickleSaveArrays/inputsArrayRun1/inputsArrayTokenized.txt\", \"wb\") as fp: # wb - Write Binary\n",
    "    pickle.dump(inputs, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
