{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kQjyTmuX7ah"
      },
      "outputs": [],
      "source": [
        "#Basics\n",
        "import os\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import glob\n",
        "import pickle\n",
        "\n",
        "#tf libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import mixed_precision\n",
        "\n",
        "#torch libraries\n",
        "import torch\n",
        "from torch import cuda\n",
        "\n",
        "#Time related libraries\n",
        "import datetime\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "#General libraries\n",
        "import collections\n",
        "import math\n",
        "import random\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Dealing with files, data and general stuff\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "#May be used but very unlikely.\n",
        "from subprocess import check_output\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from numba import jit, cuda #Former way of GPU acceleration in my project.\n",
        "\n",
        "#Transformers Library. Taken from \"Hugging Face\".\n",
        "from transformers import BertConfig, BertModel, BertTokenizer, BertForMaskedLM, BertTokenizerFast, pipeline\n",
        "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "\n",
        "#Optimizers taken from the \"Transformers\" Library that I have mentioned above as well.\n",
        "from transformers import AdamW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUrD5FjIX7al"
      },
      "outputs": [],
      "source": [
        "path_to_tokenizer = \"G:/VisualStudioCodeG/Resources/Tokenizers/bookcorpus/Run1/BERTbookcorpusTokenizerRun1-vocab.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe1JKB2WX7al",
        "outputId": "49bfe71e-33de-43f5-a568-eb206ee2f231"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BertTokenizerFast(path_to_tokenizer, max_length=512, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_l__RKMX7an"
      },
      "outputs": [],
      "source": [
        "path_to_save = \"G:/VisualStudioCodeG/Resources/PickleSaveArrays/inputsArrayRun1/text_{count}.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8_jizL9X7an"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "paths = [str(x) for x in Path('G:/VisualStudioCodeG/Resources/DataTestHuggingFace/bookcorpus/').glob('**/*.txt')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGYKvT3FX7an",
        "outputId": "685d3755-3c17-4ffd-cae3-34f8aa6d7b3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 741/741 [00:16<00:00, 45.59it/s]\n"
          ]
        }
      ],
      "source": [
        "linesA = []\n",
        "\n",
        "fileList = glob.glob('G:/VisualStudioCodeG/Resources/DataTestHuggingFace/bookcorpus' + '/*.txt')\n",
        "for filePath in tqdm(fileList):\n",
        "    with open(filePath, 'r', encoding='utf-8') as textFile:\n",
        "        linesData = textFile.read().split('\\n')\n",
        "        linesA.extend(linesData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uordVsF1X7ao",
        "outputId": "7b37b4ee-26af-479e-ee98-16d8d37156a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(fileList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIziUrO4X7ao",
        "outputId": "fdc1901c-7554-44a7-d89c-097c47369b19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "741"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25R7QzF_X7ao"
      },
      "outputs": [],
      "source": [
        "lines = []\n",
        "count = 0\n",
        "\n",
        "for path in tqdm(paths):\n",
        "    with open(path, 'r', encoding='utf-8') as textFile:\n",
        "        lines = textFile.read().split('\\n')\n",
        "    \n",
        "    inputs = tokenizer(\n",
        "    text=lines,\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    return_tensors='pt')\n",
        "\n",
        "    # Pickle a tokenized array (Save) - Splits\n",
        "    with open(f'G:/VisualStudioCodeG/Resources/PickleSaveArrays/inputsArrayRun1/tokenizedText_{count}.txt', \"wb\") as fp: # wb - Write Binary\n",
        "        pickle.dump(inputs, fp)\n",
        "        lines = []\n",
        "        count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6PfA2vAX7ap"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(\n",
        "    text=linesA,\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    padding='max_length',\n",
        "    return_tensors='pt')\n",
        "\n",
        "# Pickle a tokenized array (Save) - Whole\n",
        "with open(\"G:/VisualStudioCodeG/Resources/PickleSaveArrays/inputsArrayRun1/inputsArrayTokenized.txt\", \"wb\") as fp: # wb - Write Binary\n",
        "    pickle.dump(inputs, fp)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
